# EulerGuard Configuration File

# Path to compiled eBPF object file
bpf_path: ./bpf/main.bpf.o

# Path to detection rules file
rules_path: ./rules.yaml

# Path to log file
log_file: ./runtime.log

# Output events as JSON lines (default: false)
json_output: false

# Ring buffer size in bytes (default: 262144 = 256KB)
# Increase for high-load systems to prevent event loss
ring_buffer_size: 262144

# Process tree maximum age (default: 30m)
# Processes older than this are removed from memory
# Format: 30m, 1h, 2h30m, etc.
process_tree_max_age: 30m

# Process tree maximum size (default: 10000)
# Oldest processes are evicted when this limit is reached
process_tree_max_size: 10000

# Process tree maximum chain length (default: 50)
# Maximum number of ancestors to trace when building process chains
process_tree_max_chain_length: 50

# Log buffer size in bytes (default: 65536 = 64KB)
# Larger buffers reduce I/O but use more memory
log_buffer_size: 65536

# ============================================
# Learning Mode Configuration (CLI specific)
# ============================================

# Enable learning mode to auto-generate whitelist rules (default: false)
# When enabled, EulerGuard will observe behavior and generate rules
learn_mode: false

# Duration for learning mode (default: 5m)
# Format: 30s, 5m, 1h, etc.
learn_duration: 5m

# Output path for generated whitelist rules (default: whitelist_rules.yaml)
learn_output_path: ./whitelist_rules.yaml

# ============================================
# AI Intelligent Diagnosis Configuration (Web UI specific)
# ============================================
ai:
  # Enable AI diagnosis feature (default: true)
  enabled: true
  
  # Provider mode: "ollama" (local, recommended) or "openai" (API)
  mode: "ollama"
  
  # Ollama configuration (when mode: "ollama")
  ollama:
    # Ollama server endpoint (default assumes local Ollama installation)
    endpoint: "http://localhost:11434"
    # Model to use - qwen2.5-coder is good for technical analysis
    model: "qwen2.5-coder:1.5b"
    # Request timeout in seconds
    timeout: 60
  
  # OpenAI-compatible API configuration (when mode: "openai")
  openai:
    # API endpoint (DeepSeek, OpenAI, or any compatible service)
    endpoint: "https://api.deepseek.com"
    # API key (can also use EULERGUARD_AI_API_KEY env var)
    api_key: ""
    # Model name
    model: "deepseek-chat"
    # Request timeout in seconds
    timeout: 30
